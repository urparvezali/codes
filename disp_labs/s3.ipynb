{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afcc88e3",
   "metadata": {},
   "source": [
    "### Real-Time Speech Recognition system\n",
    "#### Theory\n",
    "Real-time speech recognition is a process where spoken language is converted into text in real-time using advanced machine learning models. The Vosk library is a popular tool for implementing speech recognition due to its lightweight design and support for multiple languages. It uses pre-trained models to recognize speech patterns and convert them into text. The process involves capturing audio input, processing it through a model, and generating text output.\n",
    "\n",
    "Key components of the system include:\n",
    "- **Audio Input Stream**: Captures real-time audio data using a microphone.\n",
    "- **Vosk Model**: A pre-trained model that processes the audio data and performs speech-to-text conversion.\n",
    "- **Recognizer**: A component that uses the model to analyze audio frames and generate results.\n",
    "- **Queue**: A data structure used to manage audio data for processing.\n",
    "\n",
    "The system also provides partial results during processing, allowing for continuous feedback before the final recognition result is generated.\n",
    "#### Objectives\n",
    "1. **Understand Real-Time Speech Recognition**: Learn how to implement a real-time speech recognition system using the Vosk library.\n",
    "2. **Explore Audio Input Handling**: Understand how to capture and process audio data using the `sounddevice` library.\n",
    "3. **Implement Speech-to-Text Conversion**: Use the Vosk model to convert spoken words into text in real-time.\n",
    "4. **Handle Partial and Final Results**: Differentiate between partial and final recognition results and display them appropriately.\n",
    "5. **Terminate on Command**: Implement functionality to stop the recognition process when a specific word (e.g., \"exit\") is spoken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22d2faeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from model/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from model/graph/HCLr.fst model/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo model/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real-time Speech Recognition started. Say exit to exit.\n",
      "Partial: this is real thing is is to cognitions his team also with continuous time listening\n",
      "Recognized: this is real thing is is to cognitions his team also with continuous time listening\n",
      "Partial: exit\n",
      "Recognized: exit\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "import sounddevice as sd\n",
    "import vosk\n",
    "import sys\n",
    "import json\n",
    "\n",
    "MODEL_PATH = \"model\"\n",
    "model = vosk.Model(MODEL_PATH)\n",
    "\n",
    "samplerate = 16000\n",
    "device = None\n",
    "q = queue.Queue()\n",
    "\n",
    "def callback(indata, frames, time, status):\n",
    "    if status:\n",
    "        print(status, file=sys.stderr)\n",
    "    q.put(bytes(indata))\n",
    "\n",
    "recognizer = vosk.KaldiRecognizer(model, samplerate)\n",
    "print(\"Real-time Speech Recognition started. Say exit to exit.\")\n",
    "with sd.RawInputStream(samplerate, 8000, device, dtype=\"int16\", channels=1, callback=callback):\n",
    "    while True:\n",
    "        data = q.get()\n",
    "        if recognizer.AcceptWaveform(data):\n",
    "            result = json.loads(recognizer.Result())\n",
    "            print(f\"\\nRecognized: {result['text']}\")\n",
    "            if result[\"text\"] == \"exit\":\n",
    "                break\n",
    "        else:\n",
    "            partial = json.loads(recognizer.PartialResult())\n",
    "            print(f\"\\rPartial: {partial['partial']}\", end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
